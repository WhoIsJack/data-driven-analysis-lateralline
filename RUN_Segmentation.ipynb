{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN: Segmentation\n",
    "\n",
    "This notebook runs the segmentations based on membrane data.\n",
    "\n",
    "The IDR data already includes the segmentations, so by default this does nothing when run. To actually rerun the pipeline when segmented images are alread present, change the `ignore_old` keyword argument in `seg.full_segmentation` to `False`. Note that the already existing segmentations will be overwritten by the new ones (though everything should stay the same unless parameters are being changed).\n",
    "\n",
    "Optionally, linear unmixing of bleed-through from a secondary channel into the membrane channel can be performed prior to segmentation. By default, this only happens for the `cxc4b:NLS-tdTomato` data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "# Generic\n",
    "from __future__ import division\n",
    "import os, sys, pickle\n",
    "import numpy as np\n",
    "\n",
    "# Internal\n",
    "from katachi.pipelines import initialization as init\n",
    "#from katachi.pipelines import segmentation as seg\n",
    "#from katachi.pipelines import feature_extraction as feat\n",
    "#from katachi.pipelines import covariate_extraction as cov\n",
    "#from katachi.utilities import loading as ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to parse relevant IDs from IDR bulk data\n",
    "\n",
    "def parse_from_IDR(dir_path, target):\n",
    "    \n",
    "    # Get all samples\n",
    "    samples = [d for d in os.listdir(dir_path) if len(d)==10\n",
    "               and os.path.isdir(os.path.join(dir_path, d))]\n",
    "    \n",
    "    # Select relevant samples\n",
    "    relevant_samples = []\n",
    "    for d in samples:\n",
    "        \n",
    "        # Get image files\n",
    "        images = [i for i in os.listdir(os.path.join(dir_path, d))\n",
    "                  if i.startswith(d) and i.endswith('.tif')]\n",
    "        \n",
    "        # Special case for membranes only\n",
    "        if target=='membranes_only':\n",
    "            if all(['lynEGFP' in img for img in images]):\n",
    "                relevant_samples.append(d)\n",
    "            \n",
    "        # All other cases\n",
    "        else:\n",
    "            if any([img.endswith(target+'.tif') for img in images]):\n",
    "                relevant_samples.append(d)\n",
    "    \n",
    "    return relevant_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment `cldnB:lyn-EGFP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target directory\n",
    "dir_path = r'data\\experimentA\\image_data'\n",
    "\n",
    "# Parse relevant IDs from IDR bulk data\n",
    "relevant_samples = parse_from_IDR(dir_path, 'membranes_only')\n",
    "print \"Found %i relevant samples!\" % len(relevant_samples)\n",
    "\n",
    "# Additional arguments \n",
    "segment_params = {'median_size'     : 3,\n",
    "                  'gaussian_sigma'  : 3,\n",
    "                  'max_offset'      : 10,\n",
    "                  'offset_step'     : 1,\n",
    "                  'clean_small'     : 1000,\n",
    "                  'clean_big'       : 1000000, \n",
    "                  'expansion_sigma' : 3}\n",
    "\n",
    "# Run segmentation pipeline\n",
    "seg.full_segmentation(dir_path, 'lynEGFP', IDs=relevant_samples,\n",
    "                      lin_unmix=False, \n",
    "                      recurse=True, ignore_old=True, processes=14,\n",
    "                      subprocesses=1, profiling=True, verbose=True,\n",
    "                      segment_params=segment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment `cldnB:lyn-EGFP + cxcr4b:NLS-tdTomato`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target directory\n",
    "dir_path = r'data\\experimentA\\image_data'\n",
    "\n",
    "# Parse relevant IDs from IDR bulk data\n",
    "relevant_samples = parse_from_IDR(dir_path, 'NLStdTomato')\n",
    "print \"Found %i relevant samples!\" % len(relevant_samples)\n",
    "\n",
    "# Additional arguments \n",
    "segment_params = {'median_size'     : 3,\n",
    "                  'gaussian_sigma'  : 3,\n",
    "                  'max_offset'      : 20,\n",
    "                  'offset_step'     : 1,\n",
    "                  'clean_small'     : 1000,\n",
    "                  'clean_big'       : 1000000, \n",
    "                  'expansion_sigma' : 3}\n",
    "unmix_params = (0.0, 1.0, 20)\n",
    "\n",
    "# Run segmentation pipeline\n",
    "seg.full_segmentation(dir_path, 'lynEGFP', IDs=relevant_samples,\n",
    "                      lin_unmix='NLStdTomato', \n",
    "                      recurse=True, ignore_old=True, processes=14,\n",
    "                      subprocesses=1, profiling=True, verbose=True,\n",
    "                      unmix_params=unmix_params, segment_params=segment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment `cldnB:lyn-EGFP + Actb2:mKate-Rab11a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target directory\n",
    "dir_path = r'data\\experimentA\\image_data'\n",
    "\n",
    "# Parse relevant IDs from IDR bulk data\n",
    "relevant_samples = parse_from_IDR(dir_path, 'mKate2rab11')\n",
    "print \"Found %i relevant samples!\" % len(relevant_samples)\n",
    "\n",
    "# Additional arguments \n",
    "segment_params = {'median_size'     : 3,\n",
    "                  'gaussian_sigma'  : 3,\n",
    "                  'max_offset'      : 40,\n",
    "                  'offset_step'     : 2,\n",
    "                  'clean_small'     : 1000,\n",
    "                  'clean_big'       : 1000000, \n",
    "                  'expansion_sigma' : 3}\n",
    "\n",
    "# Run segmentation pipeline\n",
    "seg.full_segmentation(dir_path, 'lynEGFP', IDs=relevant_samples,\n",
    "                      lin_unmix=False, \n",
    "                      recurse=True, ignore_old=True, processes=14,\n",
    "                      subprocesses=1, profiling=True, verbose=True,\n",
    "                      segment_params=segment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment `cldnB:lyn-EGFP + RNA:mKate2-Rab5a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target directory\n",
    "dir_path = r'data\\experimentA\\image_data'\n",
    "\n",
    "# Parse relevant IDs from IDR bulk data\n",
    "relevant_samples = parse_from_IDR(dir_path, 'mKate2rab5')\n",
    "print \"Found %i relevant samples!\" % len(relevant_samples)\n",
    "\n",
    "# Additional arguments \n",
    "segment_params = {'median_size'     : 3,\n",
    "                  'gaussian_sigma'  : 3,\n",
    "                  'max_offset'      : 40,\n",
    "                  'offset_step'     : 2,\n",
    "                  'clean_small'     : 1000,\n",
    "                  'clean_big'       : 1000000, \n",
    "                  'expansion_sigma' : 3}\n",
    "\n",
    "# Run segmentation pipeline\n",
    "seg.full_segmentation(dir_path, 'lynEGFP', IDs=relevant_samples,\n",
    "                      lin_unmix=False, \n",
    "                      recurse=True, ignore_old=True, processes=14,\n",
    "                      subprocesses=1, profiling=True, verbose=True,\n",
    "                      segment_params=segment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment `cldnB:lyn-EGFP + RNA:mKate2-GM130(rat)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target directory\n",
    "dir_path = r'data\\experimentA\\image_data'\n",
    "\n",
    "# Parse relevant IDs from IDR bulk data\n",
    "relevant_samples = parse_from_IDR(dir_path, 'mKate2GM130')\n",
    "print \"Found %i relevant samples!\" % len(relevant_samples)\n",
    "\n",
    "# Additional arguments \n",
    "segment_params = {'median_size'     : 3,\n",
    "                  'gaussian_sigma'  : 3,\n",
    "                  'max_offset'      : 40,\n",
    "                  'offset_step'     : 2,\n",
    "                  'clean_small'     : 1000,\n",
    "                  'clean_big'       : 1000000, \n",
    "                  'expansion_sigma' : 3}\n",
    "\n",
    "# Run segmentation pipeline\n",
    "seg.full_segmentation(dir_path, 'lynEGFP', IDs=relevant_samples,\n",
    "                      lin_unmix=False, \n",
    "                      recurse=True, ignore_old=True, processes=14,\n",
    "                      subprocesses=1, profiling=True, verbose=True,\n",
    "                      segment_params=segment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment `cldnB:lyn-EGFP + lexOP:CDMPR-tagRFPt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target directory\n",
    "dir_path = r'data\\experimentA\\image_data'\n",
    "\n",
    "# Parse relevant IDs from IDR bulk data\n",
    "relevant_samples = parse_from_IDR(dir_path, 'CDMPRtagRFPt')\n",
    "print \"Found %i relevant samples!\" % len(relevant_samples)\n",
    "\n",
    "# Additional arguments \n",
    "segment_params = {'median_size'     : 3,\n",
    "                  'gaussian_sigma'  : 3,\n",
    "                  'max_offset'      : 40,\n",
    "                  'offset_step'     : 2,\n",
    "                  'clean_small'     : 1000,\n",
    "                  'clean_big'       : 1000000, \n",
    "                  'expansion_sigma' : 3}\n",
    "\n",
    "# Run segmentation pipeline\n",
    "seg.full_segmentation(dir_path, 'lynEGFP', IDs=relevant_samples,\n",
    "                      lin_unmix=False, \n",
    "                      recurse=True, ignore_old=True, processes=14,\n",
    "                      subprocesses=1, profiling=True, verbose=True,\n",
    "                      segment_params=segment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment `cldnB:lyn-EGFP + LexOP:B4GalT1(1-55Q)-tagRFPt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target directory\n",
    "dir_path = r'data\\experimentA\\image_data'\n",
    "\n",
    "# Parse relevant IDs from IDR bulk data\n",
    "relevant_samples = parse_from_IDR(dir_path, 'b4galT1tagRFPt')\n",
    "print \"Found %i relevant samples!\" % len(relevant_samples)\n",
    "\n",
    "# Additional arguments \n",
    "segment_params = {'median_size'     : 3,\n",
    "                  'gaussian_sigma'  : 3,\n",
    "                  'max_offset'      : 40,\n",
    "                  'offset_step'     : 2,\n",
    "                  'clean_small'     : 1000,\n",
    "                  'clean_big'       : 1000000, \n",
    "                  'expansion_sigma' : 3}\n",
    "\n",
    "# Run segmentation pipeline\n",
    "seg.full_segmentation(dir_path, 'lynEGFP', IDs=relevant_samples,\n",
    "                      lin_unmix=False, \n",
    "                      recurse=True, ignore_old=True, processes=14,\n",
    "                      subprocesses=1, profiling=True, verbose=True,\n",
    "                      segment_params=segment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment `cldnB:lyn-EGFP + atoh1a:dtomato`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target directory\n",
    "dir_path = r'data\\experimentA\\image_data'\n",
    "\n",
    "# Parse relevant IDs from IDR bulk data\n",
    "relevant_samples = parse_from_IDR(dir_path, 'atoh1a')\n",
    "print \"Found %i relevant samples!\" % len(relevant_samples)\n",
    "\n",
    "# Additional arguments \n",
    "segment_params = {'median_size'     : 3,\n",
    "                  'gaussian_sigma'  : 3,\n",
    "                  'max_offset'      : 10,\n",
    "                  'offset_step'     : 1,\n",
    "                  'clean_small'     : 1000,\n",
    "                  'clean_big'       : 1000000, \n",
    "                  'expansion_sigma' : 3}\n",
    "\n",
    "# Run segmentation pipeline\n",
    "seg.full_segmentation(dir_path, 'lynEGFP', IDs=relevant_samples,\n",
    "                      lin_unmix=False, \n",
    "                      recurse=True, ignore_old=True, processes=14,\n",
    "                      subprocesses=1, profiling=True, verbose=True,\n",
    "                      segment_params=segment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment `cldnB:lyn-EGFP + 6xUAS:tagRFPt-UtrCH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target directory\n",
    "dir_path = r'data\\experimentA\\image_data'\n",
    "\n",
    "# Parse relevant IDs from IDR bulk data\n",
    "relevant_samples = parse_from_IDR(dir_path, 'tagRFPtUtrCH')\n",
    "print \"Found %i relevant samples!\" % len(relevant_samples)\n",
    "\n",
    "# Additional arguments \n",
    "segment_params = {'median_size'     : 3,\n",
    "                  'gaussian_sigma'  : 3,\n",
    "                  'max_offset'      : 10,\n",
    "                  'offset_step'     : 1,\n",
    "                  'clean_small'     : 1000,\n",
    "                  'clean_big'       : 1000000, \n",
    "                  'expansion_sigma' : 3}\n",
    "\n",
    "# Run segmentation pipeline\n",
    "seg.full_segmentation(dir_path, 'lynEGFP', IDs=relevant_samples,\n",
    "                      lin_unmix=False, \n",
    "                      recurse=True, ignore_old=True, processes=14,\n",
    "                      subprocesses=1, profiling=True, verbose=True,\n",
    "                      segment_params=segment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment `cldnB:lyn-EGFP + LysoTracker Deep Red`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target directory\n",
    "dir_path = r'data\\experimentA\\image_data'\n",
    "\n",
    "# Parse relevant IDs from IDR bulk data\n",
    "relevant_samples = parse_from_IDR(dir_path, 'lysotrackerdeepred')\n",
    "print \"Found %i relevant samples!\" % len(relevant_samples)\n",
    "\n",
    "# Additional arguments \n",
    "segment_params = {'median_size'     : 3,\n",
    "                  'gaussian_sigma'  : 3,\n",
    "                  'max_offset'      : 40,\n",
    "                  'offset_step'     : 2,\n",
    "                  'clean_small'     : 1000,\n",
    "                  'clean_big'       : 1000000, \n",
    "                  'expansion_sigma' : 3}\n",
    "\n",
    "# Run segmentation pipeline\n",
    "seg.full_segmentation(dir_path, 'lynEGFP', IDs=relevant_samples,\n",
    "                      lin_unmix=False, \n",
    "                      recurse=True, ignore_old=True, processes=14,\n",
    "                      subprocesses=1, profiling=True, verbose=True,\n",
    "                      segment_params=segment_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment `pea3 smFISH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target directory\n",
    "dir_path = r'data\\experimentB\\image_data'\n",
    "\n",
    "# Parse relevant IDs from IDR bulk data\n",
    "relevant_samples = parse_from_IDR(dir_path, 'pea3smFISH')\n",
    "print \"Found %i relevant samples!\" % len(relevant_samples)\n",
    "\n",
    "# Additional arguments \n",
    "segment_params = {'median_size'     : 3,\n",
    "                  'gaussian_sigma'  : 3,\n",
    "                  'max_offset'      : 20,\n",
    "                  'offset_step'     : 1,\n",
    "                  'clean_small'     : 1000,\n",
    "                  'clean_big'       : 1000000, \n",
    "                  'expansion_sigma' : 3}\n",
    "\n",
    "# Run segmentation pipeline\n",
    "seg.full_segmentation(dir_path, 'lynEGFP', IDs=relevant_samples,\n",
    "                      lin_unmix=False, \n",
    "                      recurse=True, ignore_old=True, processes=14,\n",
    "                      subprocesses=1, profiling=True, verbose=True,\n",
    "                      segment_params=segment_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
